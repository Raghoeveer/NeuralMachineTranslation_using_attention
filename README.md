# Neural Machine Translation in Seq2Seq Model using Attention

## Overview

This repository contains an implementation of Neural Machine Translation (NMT) using a Seq2Seq model with attention mechanisms. The model is designed to translate text from one german to english using the IWSLT2017 datset, capturing contextual information effectively through attention mechanisms.

## Features

- **Seq2Seq Architecture**: Utilizes a sequence-to-sequence model for handling variable-length input and output sequences.
- **Attention Mechanism**: Implements attention mechanisms to allow the model to focus on different parts of the input sequence during the translation process.
- **Training and Evaluation**: Provides scripts for training the model on your own dataset and evaluating its translation performance.
- **Customization**: Easily customizable for experimenting with different hyperparameters, model architectures, and datasets.

- Architecture in question:

![image](https://github.com/Raghoeveer/NeuralMachineTranslation_using_attention/assets/130668192/24e8ef85-f87a-4d6e-8682-03d06400fff0)


## Getting Started

### Prerequisites

- Python (>=3.6)
- Dependencies: Specify required libraries such as Pytorch, tqdm, datasets, spacy, numpy, tqdm etc.

### Installation

1. Clone the repository:

   ```bash
   git clone https://github.com/Raghoeveer/NeuralMachineTranslation_using_attention.git
   cd NerualMachineTranslation_using_attention

- 

